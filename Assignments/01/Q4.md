### 1. An agent that senses only partial information about the state cannot be perfectly rational.
**FALSE:** 
Even if an agent only has access to partial information, it can still be perfectly rational by doing the best it can with what it knows. For example, think of a navigation app that only receives data from a few traffic sensors; even though it doesn’t have every detail, it still calculates the best route based on the available information.

### 2. There exist task environments in which no pure reflex agent can behave rationally.
**TRUE:** 
 In some environments, a pure reflex agent which reacts only to the immediate sensory input without any memory or internal state, simply can’t be rational. Consider a rescue robot operating in a smoke-filled building: without the ability to remember or plan for unseen obstacles, a reflex-only strategy might lead it into hazards repeatedly. This shows that in complex, dynamic settings, pure reflex agents fail to behave optimally.

### 3. There exists a task environment in which every agent is rational.
**FALSE:** 
It’s unrealistic to imagine an environment where every possible agent is rational. Even in a very simple setup, some agents might behave arbitrarily or poorly due to flawed design. For instance, in a basic game where every move gives the same reward, one agent might still act randomly or inefficiently, showing that not all agents can be rational in that environment.

### 4. The input to an agent program is the same as the input to the agent function.
**FALSE:** 
The input to an agent program isn’t exactly the same as the input to the abstract agent function. The program receives raw sensor data and has to process it into a meaningful history of percepts, while the agent function itself is defined as a mapping from that percept history to actions. Think of a voice assistant: it gets raw audio data, processes it into text, and then uses that text to decide how to respond.

### 5. Every agent function is implementable by some program/machine combination.
**FALSE:**  Although we can imagine many agent functions in theory, some of them aren’t implementable because they would require infinite memory or endless computation. For example, an agent function that could perfectly solve every possible puzzle in a complex game would demand more resources than any real machine has, making it impossible to implement in practice.


### 6. Suppose an agent selects its action uniformly at random from the set of possible actions. There exists a deterministic task environment in which this agent is rational.
**TRUE:** 
 Consider a deterministic environment where every action yields exactly the same outcome—for instance, a machine that always dispenses a snack regardless of which button you press. In such an environment, if an agent chooses its action uniformly at random, it’s as rational as any other strategy since all choices lead to the same result.

### 7. It is possible for a given agent to be perfectly rational in two distinct task environments.
**TRUE:**
A well-designed agent can be perfectly rational in two different task environments if its decision-making process adapts optimally to each. Take a thermostat that consistently maintains a set temperature both in a small house and in a larger building. As long as it meets its goal in both cases, it’s acting rationally in each environment.





